# Note Technique Projet Deepfake (Update October 2023)

<aside>
üí° Note sur les aspects technico-artistiques de Deepfake

</aside>

## üé¨ Monde R√©el :

**I. R√©el √©trange** 

Le monde r√©el, tel que per√ßu √† travers les yeux de Joshua, se manifeste sous une forme d√©r√©alis√©e, fr√¥lant l'onirisme. Cette vision subjective est le c≈ìur de notre d√©marche artistique, o√π la repr√©sentation du r√©el se voit intentionnellement alt√©r√©e pour provoquer une familiarit√© troublante. L‚Äôambition est de plonger le spectateur dans une atmosph√®re √† la fois reconnue et √©trange, suscitant une r√©flexion sur l'authenticit√© de l'image.

Pour concr√©tiser cette vision, nous optons pour une technique bipartite qui associe prises de vue r√©elles et √©l√©ments 3D, visant un rendu final singulier. Les personnages, film√©s en prise de vue r√©elle, parfois s√©par√©ment, seront r√©incrust√©s ult√©rieurement pour renforcer cette impression d'un r√©el l√©g√®rement d√©cal√©. La rotoscopie, appuy√©e par EB Synth et l'intelligence artificielle de Stable Diffusion avec des plugins tels que Multicontrolnet et Wav2lip, servira de toile √† cette cr√©ation. Un micro dataset, nourri de documents en lien avec Joshua, favorisera la cr√©ation d'images hyperr√©alistes mais fluctuantes entre chaque frame, instillant un effet de morphing √† la mani√®re d'une animation, mais √† une fr√©quence inf√©rieure √† 24 images par seconde. Notre d√©marche vise ainsi √† immerger le spectateur dans une r√©alit√© biais√©e, lui faisant explorer les confins d'un monde r√©el, mais d√©concertant, √† l'image des perceptions de Joshua.

> üîç **Exemple Prise de vue r√©elle + 3D + IA**
> 
> 
> 
> [ici, la s√©quence contient un d√©cor en 3D qui est stylis√© par Stable Diffusion + controlnet. L‚Äôimage n‚Äôest pas m√©lang√©e √† la s√©quence ebsynth, mais √† l‚Äôimage de base en 3D, dont est isol√© le personnage par le masque. Cela permet de conserver beaucoup plus l‚Äôaspect original de la pi√®ce, et les couleurs (j‚Äôutilise le mode de fusion ‚Äúmultiplication‚Äù, qui modifie les couleurs, et qui peut √™tre ajust√©). Le personnage est d‚Äôabord captur√© par prise de vue r√©elle avant d‚Äô√™tre pass√© dans stable diffusion + multi controlnet. Ensuite on utilise un depht pour lui donner tri-dimensionnalit√© relative plut√¥t qu‚Äôune image plane. Le personnage existe alors dans le d√©cor, il r√©agit √† la lumi√®re ambiante etc.](Workflow%203D%20Stable%20diffusion%2008f1c680ab7344ceb8a37a96d99a44c0/Controlnet_overbase_multiply.mp4)
> 
> ici, la s√©quence contient un d√©cor en 3D qui est stylis√© par Stable Diffusion + controlnet. L‚Äôimage n‚Äôest pas m√©lang√©e √† la s√©quence ebsynth, mais √† l‚Äôimage de base en 3D, dont est isol√© le personnage par le masque. Cela permet de conserver beaucoup plus l‚Äôaspect original de la pi√®ce, et les couleurs (j‚Äôutilise le mode de fusion ‚Äúmultiplication‚Äù, qui modifie les couleurs, et qui peut √™tre ajust√©). Le personnage est d‚Äôabord captur√© par prise de vue r√©elle avant d‚Äô√™tre pass√© dans stable diffusion + multi controlnet. Ensuite on utilise un depht pour lui donner tri-dimensionnalit√© relative plut√¥t qu‚Äôune image plane. Le personnage existe alors dans le d√©cor, il r√©agit √† la lumi√®re ambiante etc.
> 

<aside>
üí° [D√©tail complet recherche workflow 3D + IA](Workflow%203D%20Stable%20diffusion%2008f1c680ab7344ceb8a37a96d99a44c0.md)

</aside>

> üîç **Exemple Prise de vue r√©elle + 3D + IA + Lipsync**
> 
> 
> [262439441-bb9d888a-d33e-4246-9f0a-1ddeac062d35 (1).mp4](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/262439441-bb9d888a-d33e-4246-9f0a-1ddeac062d35_(1).mp4)
> 
> Ici un exemple avec resynchronisation labiale.
> 

> üîç **Exemple Entra√Ænement microdataset**
> 
> 1. Images r√©elles pour alimenter le dataset
> 
> ![IMG_4057.JPG](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/IMG_4057.jpg)
> 
> 1. images g√©n√©r√©es par Stable Diffusion, guid√© par le prompt du microdataset
> 
> ![IMG_4053.JPG](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/IMG_4053.jpg)
> 

- **üí°¬†En savoir plus sur Stable Diffusion**
    
    ![278888447-483fb86d-c9a2-4c20-997c-46dafc124f25.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/278888447-483fb86d-c9a2-4c20-997c-46dafc124f25.png)
    
    **Stable Diffusion** est un mod√®le d'intelligence artificielle con√ßu pour g√©n√©rer des images √† partir de descriptions textuelles. C'est une forme de technologie appel√©e mod√®le de diffusion, qui commence par un motif de bruit al√©atoire et le transforme progressivement en une image coh√©rente au fil de plusieurs √©tapes. Pendant ce processus, le mod√®le s'aligne sur les caract√©ristiques d√©crites dans le texte fourni, permettant ainsi de cr√©er une image qui correspond √† la description.
    
    Ce mod√®le utilise √©galement des composants de l'apprentissage automatique comme **CLIP** (Contrastive Language‚ÄìImage Pretraining), qui aide l'IA √† comprendre et √† associer des textes √† des images. Cela permet √† Stable Diffusion de produire des images qui sont non seulement visuellement impressionnantes mais aussi pertinentes au texte donn√©.
    
    **Stable Diffusion** est utilis√© pour une vari√©t√© d'applications, allant de la cr√©ation artistique et du design √† des applications plus pratiques comme la g√©n√©ration d'images pour des sites web ou des publicit√©s. Sa capacit√© √† cr√©er des images d√©taill√©es et de haute qualit√© √† partir de simples descriptions textuelles en fait un outil puissant pour les cr√©ateurs de contenu, les designers et les artistes.
    
    **Automatic1111** et **ComfyUI** sont deux interfaces utilisateur populaires con√ßues pour interagir avec le mod√®le **Stable Diffusion**.
    
    ![auto_new_demo-7cc62202fb39ed2f10854a404ad127db.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/auto_new_demo-7cc62202fb39ed2f10854a404ad127db.png)
    
    **Automatic1111** est une interface web qui permet aux utilisateurs de facilement g√©n√©rer des images avec Stable Diffusion. Elle offre une vari√©t√© d'options et de param√®tres pour affiner les r√©sultats, et elle est appr√©ci√©e pour sa simplicit√© et son efficacit√©.
    
    **ComfyUI**, quant √† elle, est une autre interface qui se concentre sur une exp√©rience utilisateur agr√©able et intuitive. Elle vise √† simplifier l'interaction avec Stable Diffusion, rendant la g√©n√©ration d'images accessible m√™me pour ceux qui ne sont pas techniquement inclin√©s.
    
    ![FsBPEdJWAAA2F_G.jpg](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/FsBPEdJWAAA2F_G.jpg)
    
    Les deux interfaces visent √† rendre l'utilisation de Stable Diffusion plus accessible et plus conviviale.
    
- **üí°¬†En savoir plus sur Control Net**
    
    ![1_yjvTQ1zwgPU-PkKy4Yf0Pg.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/1_yjvTQ1zwgPU-PkKy4Yf0Pg.png)
    
    ControlNet est un plugin compl√©mentaire pour Stable Diffusion qui r√©volutionne la mani√®re dont on peut g√©n√©rer des images. Contrairement aux outils standards de g√©n√©ration d'images qui offrent peu de contr√¥le sur la composition, ControlNet permet de copier des compositions d'image ou des poses de personnages √† partir d'une image de r√©f√©rence avec une pr√©cision in√©gal√©e.
    
    Avec ControlNet, vous pouvez conserver la pose ou la position d'un personnage tout en g√©n√©rant une nouvelle image. Il peut √©galement utiliser une "depth map" pour g√©n√©rer des images qui conservent les profondeurs de l'originale, permettant de modifier l'environnement ou l'ambiance tout en conservant les caract√©ristiques principales de l'image. Enfin, ControlNet peut transformer n'importe quel croquis en une ≈ìuvre d'art plus √©labor√©e.
    
    Ce plugin change la donne en offrant plus de contr√¥le et de pr√©cision dans la cr√©ation d'images, ouvrant la porte √† de nouvelles utilisations innovantes.
    
- üí°¬†**En savoir plus sur Wave2Lip**
    
    Wave2Lip est un outil d'intelligence artificielle qui synchronise les mouvements des l√®vres d'une vid√©o avec un fichier audio donn√©. En d'autres termes, si vous avez un clip vid√©o d'une personne parlant et un enregistrement audio s√©par√© (qui pourrait √™tre une autre personne parlant ou une version modifi√©e de la voix originale), Wave2Lip peut modifier la vid√©o pour que les mouvements des l√®vres correspondent √† l'audio. Cela se fait gr√¢ce √† un r√©seau de neurones profonds qui a √©t√© form√© sur de nombreux exemples de personnes parlant.
    
    Pour l'utiliser, on fournit g√©n√©ralement √† Wave2Lip deux entr√©es : la vid√©o avec les mouvements des l√®vres originaux et l'audio que l'on souhaite synchroniser avec la vid√©o. L'algorithme analyse ensuite l'audio et g√©n√®re de nouveaux mouvements des l√®vres qui correspondent aux phon√®mes et √† la cadence de l'audio. Ces nouveaux mouvements des l√®vres sont ensuite superpos√©s sur la vid√©o originale pour cr√©er une illusion de parfaite synchronisation labiale.
    
    Il est important de noter que Wave2Lip et Stable Diffusion sont deux technologies distinctes. Stable Diffusion est un mod√®le de g√©n√©ration d'images bas√© sur des techniques de diffusion, capable de cr√©er des images r√©alistes √† partir de descriptions textuelles. Tandis que Wave2Lip se concentre sp√©cifiquement sur la synchronisation labiale dans les vid√©os.
    
    Ces technologies sont utilis√©es dans de nombreux domaines, tels que le doublage de films et de s√©ries t√©l√©vis√©es dans diff√©rentes langues, la cr√©ation d'assistants virtuels plus r√©alistes, ou m√™me la correction des erreurs de synchronisation labiale dans les enregistrements vid√©o.
    
- **üí°¬†En savoir plus sur EB synth**
    
    ![EbSynth-Advanced-Settings-Create-Glitch-Effects-with-EbSynth.webp](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/EbSynth-Advanced-Settings-Create-Glitch-Effects-with-EbSynth.webp)
    
    EbSynth est un outil de "style transfer" vid√©o, c'est-√†-dire qu'il peut appliquer le style d'une image cl√© √† une s√©quence vid√©o enti√®re. En ce qui concerne son utilisation avec Stable Diffusion et ControlNet, EbSynth peut potentiellement √™tre utilis√© pour appliquer les images g√©n√©r√©es par Stable Diffusion et modifi√©es par ControlNet √† une vid√©o. Par exemple, si vous cr√©ez une image avec Stable Diffusion qui a un style artistique particulier, et que vous utilisez ControlNet pour ajuster la composition ou les poses, vous pourriez ensuite utiliser EbSynth pour appliquer ce style et ces modifications √† chaque image d'une vid√©o, cr√©ant ainsi une s√©quence anim√©e coh√©rente avec le style et la composition d√©sir√©s. C'est une fa√ßon de combiner les forces de ces outils pour la cr√©ation de contenu vid√©o enrichi et personnalis√©.
    
- **üí°¬†En savoir plus sur Animate Diffusion**
    
    Animate Diffusion est un outil qui utilise des techniques d'intelligence artificielle pour cr√©er des animations fluides √† partir d'images fixes. Il s'appuie sur des mod√®les de diffusion, comme Stable Diffusion, pour g√©n√©rer des images interm√©diaires entre des cadres cl√©s. Cela permet de transformer une s√©rie d'images statiques en une s√©quence anim√©e coh√©rente et fluide. Animate Diffusion peut √™tre utilis√© pour animer des illustrations, des portraits, ou tout autre type d'images fixes, en leur donnant vie sous forme d'animations courtes et captivantes sans avoir recours aux m√©thodes traditionnelles d'animation image par image.
    

**II. Contenu alt√©r√©**

L'utilisation de DeepFaceLab et DeepFace Live permettra de modifier des prises de vue r√©elles existantes dans le but de cr√©er des deepfake, qui sont la mat√©rialisation des impersonations que Joshua avaient faite par texte. Elles seront ici sous forme vid√©o, gr√¢ce √† DeepFaceLab qui est un logiciel de r√©f√©rence en mati√®re de cr√©ation de deepfakes, reconnu pour sa pr√©cision et sa flexibilit√©. Quant √† DeepFace Live, il offre la possibilit√© de r√©aliser des deepfakes en temps r√©el. Ces outils seront utilis√©s pour injecter une dimension nouvelle √† des √©v√©nements historiques, offrant ainsi un regard alternatif sur des instants d√©j√† connus.

![deepfacelive_intro.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/deepfacelive_intro.png)

**III. Cr√©ation d'Archives Fictives et esth√©tique forensique**

Pour certaines sc√®nes de reconstitution forensique et aussi d‚Äôarchives disparues ou manquantes, il sera n√©cessaire de r√©interpr√©ter artistiquement la mati√®re documentaire. Pour cela, nous ferons appel √† l'intelligence artificielle et √† la mod√©lisation 3D. Le rendu sera ensuite soigneusement retravaill√© pour atteindre un degr√© de r√©alisme saisissant. Nous utiliserons des codecs vieillissants pour compresser les images et recr√©erons le style caract√©ristique d'une prise de vue amateur, afin d'imiter l'esth√©tique et les imperfections des archives d'√©poque.

- **üí°¬†En savoir plus sur workflow Reconstitution archives**
    
    [Reconstruction 3D de l‚Äôattaque](../Garland%203c4b60b393d9465c8d145df1c604c7d6/Reconstruction%203D%20de%20l%E2%80%99attaque%20e74620d265a8460e9c2f8a13c395fcb3.md)
    

## üåå Univers des Forums :

Cet univers se pr√©sente comme une transposition en 3D des forums internet, semblable √† un m√©taverse. Chaque espace refl√®te une communaut√© et une esth√©tique distincte, rappelant des jeux sociaux comme Second Life ou VR Chat. Certaines s√©quences seront enregistr√©es directement dans VR Chat ou GTA 6, tandis que d'autres seront cr√©√©es √† partir de d√©cors con√ßus par nos soins. Nous mettrons en place un syst√®me de jeu et un serveur interactif, et les s√©quences seront enregistr√©es fa√ßon machinima. Les mouvements seront captur√©s via une combinaison Xsens ou [Move.ai](http://move.ai/), et les d√©cors mod√©lis√©s dans Blender avant d'√™tre import√©s dans Unity ou Unreal. La stylisation finale se fera via Stable Diffusion en post-production.

> **üîç Exemple de sc√®ne film√©e dans VR Chat**
> 
> 
> [https://www.youtube.com/watch?v=9BbJVNiLVnU](https://www.youtube.com/watch?v=9BbJVNiLVnU)
> 
> Personnages control√©s en direct, enti√®rement avec un √©quipement VR et une tenue mocap grand public
> 

## üåÄ Esth√©tique Clip pour les S√©quences Oniriques :

Ces s√©quences serviront √† repr√©senter des √©tats psychologiques intenses comme la folie d'internet ou l'anxi√©t√© g√©n√©r√©e par le harc√®lement et les menaces de mort. Elles adopteront une forme polymorphique, psych√©d√©lique et angoissante, en utilisant notamment Deforum et Animate Diff, des plugins de Stable Diffusion, pour g√©n√©rer des vid√©os √† partir de texte ou modifier des images / vid√©o d√©j√† existantes.

- üí°¬†**En savoir plus sur Deforum et AnimateDiff**
    
    Deforum est un plugin pour Stable Diffusion qui offre une multitude d'options de personnalisation, permettant aux utilisateurs de pr√©ciser en d√©tail comment ils veulent que leurs images soient g√©n√©r√©es. Il ajoute plus de 100 param√®tres de configuration au carnet d'inference principal, permettant une grande vari√©t√© de r√©sultats et une personnalisation approfondie.
    
    Animate Diffusion, en revanche, est un outil con√ßu pour cr√©er des animations √† partir d'images fixes g√©n√©r√©es par des mod√®les de diffusion comme Stable Diffusion. Il fonctionne en g√©n√©rant des images interm√©diaires entre des images cl√©s pour cr√©er une s√©quence anim√©e fluide.
    
    La principale diff√©rence entre les deux est leur but: Deforum se concentre sur la g√©n√©ration et la personnalisation d'images fixes, tandis qu'Animate Diffusion est destin√© √† cr√©er des animations √† partir de ces images.
    

> üîç **Exemple de vid√©o Deforum**
> 
> 
> [https://www.youtube.com/watch?v=BiYCnUGoBVQ](https://www.youtube.com/watch?v=BiYCnUGoBVQ)
> 
> [https://youtu.be/yWlIfDF79Y8](https://youtu.be/yWlIfDF79Y8)
> 
> [https://youtu.be/KSuiSjov5R8?si=mK_zONOpsbIFuDJz](https://youtu.be/KSuiSjov5R8?si=mK_zONOpsbIFuDJz)
> 
> Les vid√©os sont √† jouer en 0.75x
> 
> üîç **Exemple de vid√©o AnimateDiff**
> 
> [https://x.com/rainisto/status/1709168141878640778?s=20](https://x.com/rainisto/status/1709168141878640778?s=20)
> 

## ‚úçÔ∏è¬†R√©capitulatif workflow :

![Capture d‚ÄôeÃÅcran 2023-11-01 aÃÄ 02.33.12.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/Capture_decran_2023-11-01_a_02.33.12.png)

[DeepFake_Workflow_architecture_oct_23.pdf](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/DeepFake_Workflow_architecture_oct_23.pdf)

## üì∏¬†Rendus & work in progress d√©cors :

[Orange Park Renders](Orange%20Park%20Renders%20fea190a2735642eebcc1101e3db4193d.md)

[Joshua‚Äôs bedroom 3D model](Joshua%E2%80%99s%20bedroom%203D%20model%203cf88b8bcf164add900e9b68f550e839.md) 

[](Prison%203D%20model%205ede17a4989c440d9bf1d49557048890.md) 

[Screenshots 3D process (BTS)](Screenshots%203D%20process%20(BTS)%202f69a42d074a454e8e7ee82da7c843a4.md) 

## üé• Maquette film work in progress  :

[MATSUKI THE DREAMER 29_sept_0206_IJC pour EXPORT 1_good 1_rustin_isma-goodson](https://vimeo.com/878453331/4e06e7816d?share=copy)

## üìù Conclusion :

Ce projet est une odyss√©e visuelle qui d√©fie les conventions, brouillant les fronti√®res entre r√©el et virtuel. Chaque technique et esth√©tique choisie vise √† immerger le spectateur dans l'exp√©rience subjective de Joshua, offrant une r√©flexion poignante sur la r√©alit√© digitale qui nous entoure.