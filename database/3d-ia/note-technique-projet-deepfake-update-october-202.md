# Note Technique Projet Deepfake (Update October 2023)

<aside>
ğŸ’¡ Note sur les aspects technico-artistiques de Deepfake

</aside>

## ğŸ¬ Monde RÃ©el :

**I. RÃ©el Ã©trange** 

Le monde rÃ©el, tel que perÃ§u Ã  travers les yeux de Joshua, se manifeste sous une forme dÃ©rÃ©alisÃ©e, frÃ´lant l'onirisme. Cette vision subjective est le cÅ“ur de notre dÃ©marche artistique, oÃ¹ la reprÃ©sentation du rÃ©el se voit intentionnellement altÃ©rÃ©e pour provoquer une familiaritÃ© troublante. Lâ€™ambition est de plonger le spectateur dans une atmosphÃ¨re Ã  la fois reconnue et Ã©trange, suscitant une rÃ©flexion sur l'authenticitÃ© de l'image.

Pour concrÃ©tiser cette vision, nous optons pour une technique bipartite qui associe prises de vue rÃ©elles et Ã©lÃ©ments 3D, visant un rendu final singulier. Les personnages, filmÃ©s en prise de vue rÃ©elle, parfois sÃ©parÃ©ment, seront rÃ©incrustÃ©s ultÃ©rieurement pour renforcer cette impression d'un rÃ©el lÃ©gÃ¨rement dÃ©calÃ©. La rotoscopie, appuyÃ©e par EB Synth et l'intelligence artificielle de Stable Diffusion avec des plugins tels que Multicontrolnet et Wav2lip, servira de toile Ã  cette crÃ©ation. Un micro dataset, nourri de documents en lien avec Joshua, favorisera la crÃ©ation d'images hyperrÃ©alistes mais fluctuantes entre chaque frame, instillant un effet de morphing Ã  la maniÃ¨re d'une animation, mais Ã  une frÃ©quence infÃ©rieure Ã  24 images par seconde. Notre dÃ©marche vise ainsi Ã  immerger le spectateur dans une rÃ©alitÃ© biaisÃ©e, lui faisant explorer les confins d'un monde rÃ©el, mais dÃ©concertant, Ã  l'image des perceptions de Joshua.

> ğŸ” **Exemple Prise de vue rÃ©elle + 3D + IA**
> 
> 
> 
> [ici, la sÃ©quence contient un dÃ©cor en 3D qui est stylisÃ© par Stable Diffusion + controlnet. Lâ€™image nâ€™est pas mÃ©langÃ©e Ã  la sÃ©quence ebsynth, mais Ã  lâ€™image de base en 3D, dont est isolÃ© le personnage par le masque. Cela permet de conserver beaucoup plus lâ€™aspect original de la piÃ¨ce, et les couleurs (jâ€™utilise le mode de fusion â€œmultiplicationâ€, qui modifie les couleurs, et qui peut Ãªtre ajustÃ©). Le personnage est dâ€™abord capturÃ© par prise de vue rÃ©elle avant dâ€™Ãªtre passÃ© dans stable diffusion + multi controlnet. Ensuite on utilise un depht pour lui donner tri-dimensionnalitÃ© relative plutÃ´t quâ€™une image plane. Le personnage existe alors dans le dÃ©cor, il rÃ©agit Ã  la lumiÃ¨re ambiante etc.](Workflow%203D%20Stable%20diffusion%2008f1c680ab7344ceb8a37a96d99a44c0/Controlnet_overbase_multiply.mp4)
> 
> ici, la sÃ©quence contient un dÃ©cor en 3D qui est stylisÃ© par Stable Diffusion + controlnet. Lâ€™image nâ€™est pas mÃ©langÃ©e Ã  la sÃ©quence ebsynth, mais Ã  lâ€™image de base en 3D, dont est isolÃ© le personnage par le masque. Cela permet de conserver beaucoup plus lâ€™aspect original de la piÃ¨ce, et les couleurs (jâ€™utilise le mode de fusion â€œmultiplicationâ€, qui modifie les couleurs, et qui peut Ãªtre ajustÃ©). Le personnage est dâ€™abord capturÃ© par prise de vue rÃ©elle avant dâ€™Ãªtre passÃ© dans stable diffusion + multi controlnet. Ensuite on utilise un depht pour lui donner tri-dimensionnalitÃ© relative plutÃ´t quâ€™une image plane. Le personnage existe alors dans le dÃ©cor, il rÃ©agit Ã  la lumiÃ¨re ambiante etc.
> 

<aside>
ğŸ’¡ [DÃ©tail complet recherche workflow 3D + IA](Workflow%203D%20Stable%20diffusion%2008f1c680ab7344ceb8a37a96d99a44c0.md)

</aside>

> ğŸ” **Exemple Prise de vue rÃ©elle + 3D + IA + Lipsync**
> 
> 
> [262439441-bb9d888a-d33e-4246-9f0a-1ddeac062d35 (1).mp4](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/262439441-bb9d888a-d33e-4246-9f0a-1ddeac062d35_(1).mp4)
> 
> Ici un exemple avec resynchronisation labiale.
> 

> ğŸ” **Exemple EntraÃ®nement microdataset**
> 
> 1. Images rÃ©elles pour alimenter le dataset
> 
> ![IMG_4057.JPG](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/IMG_4057.jpg)
> 
> 1. images gÃ©nÃ©rÃ©es par Stable Diffusion, guidÃ© par le prompt du microdataset
> 
> ![IMG_4053.JPG](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/IMG_4053.jpg)
> 

- **ğŸ’¡Â En savoir plus sur Stable Diffusion**
    
    ![278888447-483fb86d-c9a2-4c20-997c-46dafc124f25.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/278888447-483fb86d-c9a2-4c20-997c-46dafc124f25.png)
    
    **Stable Diffusion** est un modÃ¨le d'intelligence artificielle conÃ§u pour gÃ©nÃ©rer des images Ã  partir de descriptions textuelles. C'est une forme de technologie appelÃ©e modÃ¨le de diffusion, qui commence par un motif de bruit alÃ©atoire et le transforme progressivement en une image cohÃ©rente au fil de plusieurs Ã©tapes. Pendant ce processus, le modÃ¨le s'aligne sur les caractÃ©ristiques dÃ©crites dans le texte fourni, permettant ainsi de crÃ©er une image qui correspond Ã  la description.
    
    Ce modÃ¨le utilise Ã©galement des composants de l'apprentissage automatique comme **CLIP** (Contrastive Languageâ€“Image Pretraining), qui aide l'IA Ã  comprendre et Ã  associer des textes Ã  des images. Cela permet Ã  Stable Diffusion de produire des images qui sont non seulement visuellement impressionnantes mais aussi pertinentes au texte donnÃ©.
    
    **Stable Diffusion** est utilisÃ© pour une variÃ©tÃ© d'applications, allant de la crÃ©ation artistique et du design Ã  des applications plus pratiques comme la gÃ©nÃ©ration d'images pour des sites web ou des publicitÃ©s. Sa capacitÃ© Ã  crÃ©er des images dÃ©taillÃ©es et de haute qualitÃ© Ã  partir de simples descriptions textuelles en fait un outil puissant pour les crÃ©ateurs de contenu, les designers et les artistes.
    
    **Automatic1111** et **ComfyUI** sont deux interfaces utilisateur populaires conÃ§ues pour interagir avec le modÃ¨le **Stable Diffusion**.
    
    ![auto_new_demo-7cc62202fb39ed2f10854a404ad127db.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/auto_new_demo-7cc62202fb39ed2f10854a404ad127db.png)
    
    **Automatic1111** est une interface web qui permet aux utilisateurs de facilement gÃ©nÃ©rer des images avec Stable Diffusion. Elle offre une variÃ©tÃ© d'options et de paramÃ¨tres pour affiner les rÃ©sultats, et elle est apprÃ©ciÃ©e pour sa simplicitÃ© et son efficacitÃ©.
    
    **ComfyUI**, quant Ã  elle, est une autre interface qui se concentre sur une expÃ©rience utilisateur agrÃ©able et intuitive. Elle vise Ã  simplifier l'interaction avec Stable Diffusion, rendant la gÃ©nÃ©ration d'images accessible mÃªme pour ceux qui ne sont pas techniquement inclinÃ©s.
    
    ![FsBPEdJWAAA2F_G.jpg](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/FsBPEdJWAAA2F_G.jpg)
    
    Les deux interfaces visent Ã  rendre l'utilisation de Stable Diffusion plus accessible et plus conviviale.
    
- **ğŸ’¡Â En savoir plus sur Control Net**
    
    ![1_yjvTQ1zwgPU-PkKy4Yf0Pg.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/1_yjvTQ1zwgPU-PkKy4Yf0Pg.png)
    
    ControlNet est un plugin complÃ©mentaire pour Stable Diffusion qui rÃ©volutionne la maniÃ¨re dont on peut gÃ©nÃ©rer des images. Contrairement aux outils standards de gÃ©nÃ©ration d'images qui offrent peu de contrÃ´le sur la composition, ControlNet permet de copier des compositions d'image ou des poses de personnages Ã  partir d'une image de rÃ©fÃ©rence avec une prÃ©cision inÃ©galÃ©e.
    
    Avec ControlNet, vous pouvez conserver la pose ou la position d'un personnage tout en gÃ©nÃ©rant une nouvelle image. Il peut Ã©galement utiliser une "depth map" pour gÃ©nÃ©rer des images qui conservent les profondeurs de l'originale, permettant de modifier l'environnement ou l'ambiance tout en conservant les caractÃ©ristiques principales de l'image. Enfin, ControlNet peut transformer n'importe quel croquis en une Å“uvre d'art plus Ã©laborÃ©e.
    
    Ce plugin change la donne en offrant plus de contrÃ´le et de prÃ©cision dans la crÃ©ation d'images, ouvrant la porte Ã  de nouvelles utilisations innovantes.
    
- ğŸ’¡Â **En savoir plus sur Wave2Lip**
    
    Wave2Lip est un outil d'intelligence artificielle qui synchronise les mouvements des lÃ¨vres d'une vidÃ©o avec un fichier audio donnÃ©. En d'autres termes, si vous avez un clip vidÃ©o d'une personne parlant et un enregistrement audio sÃ©parÃ© (qui pourrait Ãªtre une autre personne parlant ou une version modifiÃ©e de la voix originale), Wave2Lip peut modifier la vidÃ©o pour que les mouvements des lÃ¨vres correspondent Ã  l'audio. Cela se fait grÃ¢ce Ã  un rÃ©seau de neurones profonds qui a Ã©tÃ© formÃ© sur de nombreux exemples de personnes parlant.
    
    Pour l'utiliser, on fournit gÃ©nÃ©ralement Ã  Wave2Lip deux entrÃ©es : la vidÃ©o avec les mouvements des lÃ¨vres originaux et l'audio que l'on souhaite synchroniser avec la vidÃ©o. L'algorithme analyse ensuite l'audio et gÃ©nÃ¨re de nouveaux mouvements des lÃ¨vres qui correspondent aux phonÃ¨mes et Ã  la cadence de l'audio. Ces nouveaux mouvements des lÃ¨vres sont ensuite superposÃ©s sur la vidÃ©o originale pour crÃ©er une illusion de parfaite synchronisation labiale.
    
    Il est important de noter que Wave2Lip et Stable Diffusion sont deux technologies distinctes. Stable Diffusion est un modÃ¨le de gÃ©nÃ©ration d'images basÃ© sur des techniques de diffusion, capable de crÃ©er des images rÃ©alistes Ã  partir de descriptions textuelles. Tandis que Wave2Lip se concentre spÃ©cifiquement sur la synchronisation labiale dans les vidÃ©os.
    
    Ces technologies sont utilisÃ©es dans de nombreux domaines, tels que le doublage de films et de sÃ©ries tÃ©lÃ©visÃ©es dans diffÃ©rentes langues, la crÃ©ation d'assistants virtuels plus rÃ©alistes, ou mÃªme la correction des erreurs de synchronisation labiale dans les enregistrements vidÃ©o.
    
- **ğŸ’¡Â En savoir plus sur EB synth**
    
    ![EbSynth-Advanced-Settings-Create-Glitch-Effects-with-EbSynth.webp](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/EbSynth-Advanced-Settings-Create-Glitch-Effects-with-EbSynth.webp)
    
    EbSynth est un outil de "style transfer" vidÃ©o, c'est-Ã -dire qu'il peut appliquer le style d'une image clÃ© Ã  une sÃ©quence vidÃ©o entiÃ¨re. En ce qui concerne son utilisation avec Stable Diffusion et ControlNet, EbSynth peut potentiellement Ãªtre utilisÃ© pour appliquer les images gÃ©nÃ©rÃ©es par Stable Diffusion et modifiÃ©es par ControlNet Ã  une vidÃ©o. Par exemple, si vous crÃ©ez une image avec Stable Diffusion qui a un style artistique particulier, et que vous utilisez ControlNet pour ajuster la composition ou les poses, vous pourriez ensuite utiliser EbSynth pour appliquer ce style et ces modifications Ã  chaque image d'une vidÃ©o, crÃ©ant ainsi une sÃ©quence animÃ©e cohÃ©rente avec le style et la composition dÃ©sirÃ©s. C'est une faÃ§on de combiner les forces de ces outils pour la crÃ©ation de contenu vidÃ©o enrichi et personnalisÃ©.
    
- **ğŸ’¡Â En savoir plus sur Animate Diffusion**
    
    Animate Diffusion est un outil qui utilise des techniques d'intelligence artificielle pour crÃ©er des animations fluides Ã  partir d'images fixes. Il s'appuie sur des modÃ¨les de diffusion, comme Stable Diffusion, pour gÃ©nÃ©rer des images intermÃ©diaires entre des cadres clÃ©s. Cela permet de transformer une sÃ©rie d'images statiques en une sÃ©quence animÃ©e cohÃ©rente et fluide. Animate Diffusion peut Ãªtre utilisÃ© pour animer des illustrations, des portraits, ou tout autre type d'images fixes, en leur donnant vie sous forme d'animations courtes et captivantes sans avoir recours aux mÃ©thodes traditionnelles d'animation image par image.
    

**II. Contenu altÃ©rÃ©**

L'utilisation de DeepFaceLab et DeepFace Live permettra de modifier des prises de vue rÃ©elles existantes dans le but de crÃ©er des deepfake, qui sont la matÃ©rialisation des impersonations que Joshua avaient faite par texte. Elles seront ici sous forme vidÃ©o, grÃ¢ce Ã  DeepFaceLab qui est un logiciel de rÃ©fÃ©rence en matiÃ¨re de crÃ©ation de deepfakes, reconnu pour sa prÃ©cision et sa flexibilitÃ©. Quant Ã  DeepFace Live, il offre la possibilitÃ© de rÃ©aliser des deepfakes en temps rÃ©el. Ces outils seront utilisÃ©s pour injecter une dimension nouvelle Ã  des Ã©vÃ©nements historiques, offrant ainsi un regard alternatif sur des instants dÃ©jÃ  connus.

![deepfacelive_intro.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/deepfacelive_intro.png)

**III. CrÃ©ation d'Archives Fictives et esthÃ©tique forensique**

Pour certaines scÃ¨nes de reconstitution forensique et aussi dâ€™archives disparues ou manquantes, il sera nÃ©cessaire de rÃ©interprÃ©ter artistiquement la matiÃ¨re documentaire. Pour cela, nous ferons appel Ã  l'intelligence artificielle et Ã  la modÃ©lisation 3D. Le rendu sera ensuite soigneusement retravaillÃ© pour atteindre un degrÃ© de rÃ©alisme saisissant. Nous utiliserons des codecs vieillissants pour compresser les images et recrÃ©erons le style caractÃ©ristique d'une prise de vue amateur, afin d'imiter l'esthÃ©tique et les imperfections des archives d'Ã©poque.

- **ğŸ’¡Â En savoir plus sur workflow Reconstitution archives**
    
    [Reconstruction 3D de lâ€™attaque](../Garland%203c4b60b393d9465c8d145df1c604c7d6/Reconstruction%203D%20de%20l%E2%80%99attaque%20e74620d265a8460e9c2f8a13c395fcb3.md)
    

## ğŸŒŒ Univers des Forums :

Cet univers se prÃ©sente comme une transposition en 3D des forums internet, semblable Ã  un mÃ©taverse. Chaque espace reflÃ¨te une communautÃ© et une esthÃ©tique distincte, rappelant des jeux sociaux comme Second Life ou VR Chat. Certaines sÃ©quences seront enregistrÃ©es directement dans VR Chat ou GTA 6, tandis que d'autres seront crÃ©Ã©es Ã  partir de dÃ©cors conÃ§us par nos soins. Nous mettrons en place un systÃ¨me de jeu et un serveur interactif, et les sÃ©quences seront enregistrÃ©es faÃ§on machinima. Les mouvements seront capturÃ©s via une combinaison Xsens ou [Move.ai](http://move.ai/), et les dÃ©cors modÃ©lisÃ©s dans Blender avant d'Ãªtre importÃ©s dans Unity ou Unreal. La stylisation finale se fera via Stable Diffusion en post-production.

> **ğŸ” Exemple de scÃ¨ne filmÃ©e dans VR Chat**
> 
> 
> [https://www.youtube.com/watch?v=9BbJVNiLVnU](https://www.youtube.com/watch?v=9BbJVNiLVnU)
> 
> Personnages controlÃ©s en direct, entiÃ¨rement avec un Ã©quipement VR et une tenue mocap grand public
> 

## ğŸŒ€ EsthÃ©tique Clip pour les SÃ©quences Oniriques :

Ces sÃ©quences serviront Ã  reprÃ©senter des Ã©tats psychologiques intenses comme la folie d'internet ou l'anxiÃ©tÃ© gÃ©nÃ©rÃ©e par le harcÃ¨lement et les menaces de mort. Elles adopteront une forme polymorphique, psychÃ©dÃ©lique et angoissante, en utilisant notamment Deforum et Animate Diff, des plugins de Stable Diffusion, pour gÃ©nÃ©rer des vidÃ©os Ã  partir de texte ou modifier des images / vidÃ©o dÃ©jÃ  existantes.

- ğŸ’¡Â **En savoir plus sur Deforum et AnimateDiff**
    
    Deforum est un plugin pour Stable Diffusion qui offre une multitude d'options de personnalisation, permettant aux utilisateurs de prÃ©ciser en dÃ©tail comment ils veulent que leurs images soient gÃ©nÃ©rÃ©es. Il ajoute plus de 100 paramÃ¨tres de configuration au carnet d'inference principal, permettant une grande variÃ©tÃ© de rÃ©sultats et une personnalisation approfondie.
    
    Animate Diffusion, en revanche, est un outil conÃ§u pour crÃ©er des animations Ã  partir d'images fixes gÃ©nÃ©rÃ©es par des modÃ¨les de diffusion comme Stable Diffusion. Il fonctionne en gÃ©nÃ©rant des images intermÃ©diaires entre des images clÃ©s pour crÃ©er une sÃ©quence animÃ©e fluide.
    
    La principale diffÃ©rence entre les deux est leur but: Deforum se concentre sur la gÃ©nÃ©ration et la personnalisation d'images fixes, tandis qu'Animate Diffusion est destinÃ© Ã  crÃ©er des animations Ã  partir de ces images.
    

> ğŸ” **Exemple de vidÃ©o Deforum**
> 
> 
> [https://www.youtube.com/watch?v=BiYCnUGoBVQ](https://www.youtube.com/watch?v=BiYCnUGoBVQ)
> 
> [https://youtu.be/yWlIfDF79Y8](https://youtu.be/yWlIfDF79Y8)
> 
> [https://youtu.be/KSuiSjov5R8?si=mK_zONOpsbIFuDJz](https://youtu.be/KSuiSjov5R8?si=mK_zONOpsbIFuDJz)
> 
> Les vidÃ©os sont Ã  jouer en 0.75x
> 
> ğŸ” **Exemple de vidÃ©o AnimateDiff**
> 
> [https://x.com/rainisto/status/1709168141878640778?s=20](https://x.com/rainisto/status/1709168141878640778?s=20)
> 

## âœï¸Â RÃ©capitulatif workflow :

![Capture dâ€™eÌcran 2023-11-01 aÌ€ 02.33.12.png](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/Capture_decran_2023-11-01_a_02.33.12.png)

[DeepFake_Workflow_architecture_oct_23.pdf](Note%20Technique%20Projet%20Deepfake%20(Update%20October%20202%20eff67243e5d64c59a3a9bd41693174d3/DeepFake_Workflow_architecture_oct_23.pdf)

## ğŸ“¸Â Rendus & work in progress dÃ©cors :

[Orange Park Renders](Orange%20Park%20Renders%20fea190a2735642eebcc1101e3db4193d.md)

[Joshuaâ€™s bedroom 3D model](Joshua%E2%80%99s%20bedroom%203D%20model%203cf88b8bcf164add900e9b68f550e839.md) 

[](Prison%203D%20model%205ede17a4989c440d9bf1d49557048890.md) 

[Screenshots 3D process (BTS)](Screenshots%203D%20process%20(BTS)%202f69a42d074a454e8e7ee82da7c843a4.md) 

## ğŸ¥ Maquette film work in progress  :

[MATSUKI THE DREAMER 29_sept_0206_IJC pour EXPORT 1_good 1_rustin_isma-goodson](https://vimeo.com/878453331/4e06e7816d?share=copy)

## ğŸ“ Conclusion :

Ce projet est une odyssÃ©e visuelle qui dÃ©fie les conventions, brouillant les frontiÃ¨res entre rÃ©el et virtuel. Chaque technique et esthÃ©tique choisie vise Ã  immerger le spectateur dans l'expÃ©rience subjective de Joshua, offrant une rÃ©flexion poignante sur la rÃ©alitÃ© digitale qui nous entoure.